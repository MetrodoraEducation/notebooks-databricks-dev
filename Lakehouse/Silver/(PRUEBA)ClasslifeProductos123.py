# Databricks notebook source
# DBTITLE 1,ulac
# MAGIC %run "../Silver/configuration"

# COMMAND ----------

endpoint_process_name = "enroll_groups"
table_name = "JsaClassLifeProductos"

classlifetitulaciones_df = spark.read.json(f"{bronze_folder_path}/lakehouse/classlife/{endpoint_process_name}/{current_date}/{table_name}.json")
classlifetitulaciones_df

# COMMAND ----------

# MAGIC %md
# MAGIC **Pasos principales:**
# MAGIC - Limpiar el esquema completo del DataFrame antes de trabajar con las columnas anidadas.
# MAGIC - Desanidar las estructuras una por una asegurando que no existan conflictos.
# MAGIC - Revisar si existen estructuras complejas adicionales que deban manejarse de forma especial.

# COMMAND ----------

# üìå Listar columnas actuales antes de desanidar
print("üìå Columnas iniciales en el DataFrame:")
print(classlifetitulaciones_df.columns)

# COMMAND ----------

# DBTITLE 1,Desanida data
from pyspark.sql.functions import col, explode
from pyspark.sql.types import ArrayType, StructType

# üìå Verificar si `data` existe en el DataFrame
if "data" in classlifetitulaciones_df.columns:
    print("\n‚úÖ 'data' detectado. Procedemos a analizar su tipo.")

    # Obtener el tipo de `data`
    data_type = classlifetitulaciones_df.schema["data"].dataType

    # üìå Si `data` es un ARRAY de estructuras, explotar antes de desanidar
    if isinstance(data_type, ArrayType) and isinstance(data_type.elementType, StructType):
        print("\n‚úÖ 'data' es un ARRAY de estructuras. Procedemos a explotar.")
        classlifetitulaciones_df = classlifetitulaciones_df.withColumn("data", explode(col("data")))

    # üìå Extraer subcolumnas de `data`
    print("\n‚úÖ Extrayendo subcolumnas de 'data'")
    data_cols = classlifetitulaciones_df.select("data.*").columns

    # üìå Crear nuevo DataFrame con `data.*` extra√≠do
    classlifetitulaciones_df = classlifetitulaciones_df.select(
        "*",  # Mantiene todas las columnas originales
        *[col(f"data.{c}").alias(c) for c in data_cols]  # Extrae columnas internas de `data`
    ).drop("data")  # Elimina `data` despu√©s de extraer sus valores

    print("\n‚úÖ 'data' ha sido explotado y desanidado con √©xito.")

else:
    print("\n‚ö†Ô∏è 'data' NO encontrado en el DataFrame.")

# üìå Inspeccionar resultado final
print("\nüìå Esquema despu√©s del desanidado de `data`:")
classlifetitulaciones_df.printSchema()

# üìå Mostrar resultado final
display(classlifetitulaciones_df)

# COMMAND ----------

# üìå Inspeccionar despu√©s del desanidado
print("\nüìå Esquema despu√©s del desanidado de `data`:")
classlifetitulaciones_df.printSchema()

# üìå Mostrar resultado final
display(classlifetitulaciones_df)


# COMMAND ----------

from pyspark.sql.functions import col, explode
from pyspark.sql.types import ArrayType, StructType

# üìå Verificar si `items` existe en el DataFrame
if "items" in classlifetitulaciones_df.columns:
    print("\n‚úÖ 'items' detectado. Procedemos a analizar su tipo.")

    # Obtener tipo de datos de `items`
    items_type = classlifetitulaciones_df.schema["items"].dataType

    # üìå Si `items` es un array de estructuras, lo explotamos
    if isinstance(items_type, ArrayType) and isinstance(items_type.elementType, StructType):
        print("\n‚úÖ 'items' es un ARRAY de estructuras. Procedemos a explotar.")
        
        # Explotamos `items`
        classlifetitulaciones_df = classlifetitulaciones_df.withColumn("items", explode(col("items")))

        # üìå Extraer columnas internas de `items`
        items_cols = classlifetitulaciones_df.select("items.*").columns

        # üìå Renombrar las columnas y agregarlas al DataFrame
        classlifetitulaciones_df = classlifetitulaciones_df.select(
            "*",  # Mantiene todas las columnas originales
            *[col(f"items.{c}").alias(f"items_{c}") for c in items_cols]
        ).drop("items")  # Elimina `items` original despu√©s de extraer sus valores

        print("\n‚úÖ 'items' ha sido explotado y desanidado con √©xito.")
    
    else:
        print("\n‚ö†Ô∏è 'items' NO es un array de estructuras. No se realizar√° ninguna transformaci√≥n.")
else:
    print("\n‚ö†Ô∏è 'items' NO encontrado en el DataFrame.")

display(classlifetitulaciones_df)

# COMMAND ----------

# DBTITLE 1,Desanida counters
from pyspark.sql.functions import col
from pyspark.sql.types import StructType

# üìå Verificar si `counters` existe y es una estructura antes de desanidar
if "counters" in classlifetitulaciones_df.columns and isinstance(classlifetitulaciones_df.schema["counters"].dataType, StructType):
    print("üìå 'counters' es una estructura. Procedemos a desanidar.")

    # üìå Obtener subcolumnas de `counters`, excluyendo `enroll_group_id`
    counters_cols = [
        c for c in classlifetitulaciones_df.select("counters.*").columns if c != "enroll_group_id"
    ]

    if counters_cols:
        # üìå Extraer y renombrar las columnas de `counters`
        classlifetitulaciones_df = classlifetitulaciones_df.select(
            "*",  # Mantiene todas las columnas originales
            *[col(f"counters.{c}").alias(f"counters_{c}") for c in counters_cols]  # Renombrar subcolumnas
        )

    # üìå Eliminar `counters` original despu√©s de extraer sus datos
    classlifetitulaciones_df = classlifetitulaciones_df.drop("counters")

# üìå Mostrar resultado final
display(classlifetitulaciones_df)


# COMMAND ----------

# üìå Inspeccionar despu√©s de desanidar `items`
print("üìå Esquema despu√©s de desanidar `items` y 'counters':")
classlifetitulaciones_df.printSchema()


# üìå Aplicar limpieza de nombres de columnas
classlifetitulaciones_df = clean_column_names(classlifetitulaciones_df)

display(classlifetitulaciones_df)

# COMMAND ----------

# üìå Inspeccionar despu√©s de limpiar nombres de columnas
print("üìå Esquema despu√©s de limpiar nombres de columnas:")
classlifetitulaciones_df.printSchema()

display(classlifetitulaciones_df)

# COMMAND ----------

# üìå Desanidar estructuras internas (counters, metas) si existen
if "counters" in classlifetitulaciones_df.columns:
    counters_cols = classlifetitulaciones_df.select("counters.*").columns
    classlifetitulaciones_df = classlifetitulaciones_df.select("*", *[col(f"counters.{c}").alias(f"counters_{c}") for c in counters_cols]).drop("counters")

if "metas" in classlifetitulaciones_df.columns:
    metas_cols = classlifetitulaciones_df.select("metas.*").columns
    classlifetitulaciones_df = classlifetitulaciones_df.select("*", *[col(f"metas.{c}").alias(f"metas_{c}") for c in metas_cols]).drop("metas")

display(classlifetitulaciones_df)

# COMMAND ----------

# üìå Inspeccionar despu√©s de expandir estructuras internas
print("üìå Esquema final despu√©s de desanidar estructuras:")
classlifetitulaciones_df.printSchema()

display(classlifetitulaciones_df)

# COMMAND ----------

from pyspark.sql.functions import col

def clean_and_keep_one_enroll_group_id(df):
    """
    - Asegura que solo haya una columna `enroll_group_id` en el DataFrame.
    - Si existen `enroll_group_id_2`, `enroll_group_id_3`, etc., las elimina.
    - Mantiene solo la primera aparici√≥n de `enroll_group_id`.
    """
    
    # üìå Detectar columnas duplicadas antes de realizar operaciones
    column_counts = {}
    for col_name in df.columns:
        column_counts[col_name] = column_counts.get(col_name, 0) + 1

    # üìå Identificar columnas `enroll_group_id` duplicadas
    final_columns = []
    enroll_group_id_seen = False  # Controla si ya tomamos `enroll_group_id`

    for col_name in df.columns:
        if col_name == "enroll_group_id":
            if enroll_group_id_seen:
                print(f"‚ö†Ô∏è Eliminando columna duplicada: {col_name}")
                continue  # Evita agregar m√°s de una vez `enroll_group_id`
            enroll_group_id_seen = True

        elif col_name.startswith("enroll_group_id_"):  # Si hay versiones duplicadas, eliminarlas
            print(f"‚ö†Ô∏è Eliminando columna extra: {col_name}")
            continue

        # Agregar columna solo si no es una duplicada de `enroll_group_id`
        final_columns.append(col_name)

    # üìå Seleccionar solo las columnas que queremos conservar
    df = df.select(*[col(c) for c in final_columns])

    return df

# üöÄ **ANTES DE HACER CUALQUIER OTRA OPERACI√ìN, eliminamos duplicados**
classlifetitulaciones_df = clean_and_keep_one_enroll_group_id(classlifetitulaciones_df)

# üìå Verificar que solo queda una `enroll_group_id`
assert classlifetitulaciones_df.columns.count("enroll_group_id") == 1, "‚ùå Hay m√°s de una `enroll_group_id`"

print("\n‚úÖ Se ha eliminado cualquier duplicado de `enroll_group_id` correctamente.")

# üìå Mostrar los primeros registros
display(classlifetitulaciones_df)


# COMMAND ----------

from pyspark.sql.functions import col, to_date, to_timestamp, lit, current_timestamp
from pyspark.sql.types import StringType, IntegerType, DoubleType

# üìå Verificar columnas duplicadas
column_counts = {col_name: classlifetitulaciones_df.columns.count(col_name) for col_name in set(classlifetitulaciones_df.columns)}
duplicated_columns = [col_name for col_name, count in column_counts.items() if count > 1]

if duplicated_columns:
    print(f"‚ö†Ô∏è Columnas duplicadas detectadas: {duplicated_columns}")

    # Eliminar duplicados conservando solo la primera aparici√≥n
    selected_columns = []
    seen_columns = set()

    for col_name in classlifetitulaciones_df.columns:
        if col_name not in seen_columns:
            seen_columns.add(col_name)
            selected_columns.append(col(col_name))

    # Filtrar el DataFrame con solo las columnas √∫nicas
    classlifetitulaciones_df = classlifetitulaciones_df.select(*selected_columns)



# COMMAND ----------

classlifetitulaciones_df.createOrReplaceTempView("classlifetitulaciones_view")

# COMMAND ----------

# MAGIC %sql
# MAGIC MERGE INTO silver_lakehouse.classlifetitulaciones AS target
# MAGIC USING classlifetitulaciones_view AS source
# MAGIC ON target.enroll_group_id = source.enroll_group_id
# MAGIC WHEN MATCHED THEN 
# MAGIC     UPDATE SET *
# MAGIC WHEN NOT MATCHED THEN 
# MAGIC     INSERT *;

# COMMAND ----------

# MAGIC %sql
# MAGIC select * from silver_lakehouse.classlifetitulaciones
